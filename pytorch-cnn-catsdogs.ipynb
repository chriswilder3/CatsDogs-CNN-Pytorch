{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1003830,"sourceType":"datasetVersion","datasetId":550917}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-16T19:44:00.913112Z","iopub.execute_input":"2024-04-16T19:44:00.913544Z","iopub.status.idle":"2024-04-16T19:44:06.895061Z","shell.execute_reply.started":"2024-04-16T19:44:00.913510Z","shell.execute_reply":"2024-04-16T19:44:06.893669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tqdm\n!pip install cv2","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:25:37.940238Z","iopub.execute_input":"2024-04-16T18:25:37.940661Z","iopub.status.idle":"2024-04-16T18:25:54.859629Z","shell.execute_reply.started":"2024-04-16T18:25:37.940619Z","shell.execute_reply":"2024-04-16T18:25:54.858413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport cv2 # OPENCV keep it as it is to avoid confusion among versions\nimport os\nfrom tqdm import tqdm # Helps show progress on loops","metadata":{"execution":{"iopub.status.busy":"2024-04-17T08:28:35.095496Z","iopub.execute_input":"2024-04-17T08:28:35.095958Z","iopub.status.idle":"2024-04-17T08:28:35.400643Z","shell.execute_reply.started":"2024-04-17T08:28:35.095925Z","shell.execute_reply":"2024-04-17T08:28:35.399041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread('/kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Cat/0.jpg')\n\nimport matplotlib.pyplot as plt\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T08:28:38.616373Z","iopub.execute_input":"2024-04-17T08:28:38.616824Z","iopub.status.idle":"2024-04-17T08:28:39.126568Z","shell.execute_reply.started":"2024-04-17T08:28:38.616792Z","shell.execute_reply":"2024-04-17T08:28:39.125274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CatsDogs():\n    catpath = '/kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Cat'\n    dogpath = '/kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog'\n    imgSize = 50\n    nCats = 0\n    nDogs = 0\n    training_data = []\n    \n    def getData(self):\n        # Get all cat images\n        for i in tqdm(os.listdir(path = self.catpath)):\n            filepath = os.path.join(self.catpath,i)\n            try:\n                img = cv2.imread(filepath,cv2.IMREAD_GRAYSCALE)\n                img = cv2.resize(img, (self.imgSize, self.imgSize))\n                label = np.eye(2)[0] # Assume cat has OHE vector [1, 0]\n                self.training_data.append([np.array(img),label])\n                self.nCats += 1\n            except:\n                pass\n        # Get all dog Images\n        for i in tqdm(os.listdir(path = self.dogpath)):\n            filepath = os.path.join(self.dogpath,i)\n            try:\n                img = cv2.imread(filepath,cv2.IMREAD_GRAYSCALE)\n                img = cv2.resize(img, (self.imgSize, self.imgSize))\n                label = np.eye(2)[1] # Assume dog has OHE vector [0, 1]\n                self.training_data.append([np.array(img),label])\n                self.nDogs += 1\n            except Exception as e:\n                pass\n        print(\"hello\")\n        #print(self.training_data)\n        np.random.shuffle(self.training_data)\n        np.save(\"train_data.npy\", np.array(self.training_data, dtype=object))\n        return self.training_data\nobj = CatsDogs()\nd = obj.getData()","metadata":{"execution":{"iopub.status.busy":"2024-04-17T08:38:01.808351Z","iopub.execute_input":"2024-04-17T08:38:01.808860Z","iopub.status.idle":"2024-04-17T08:39:06.835213Z","shell.execute_reply.started":"2024-04-17T08:38:01.808821Z","shell.execute_reply":"2024-04-17T08:39:06.833500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"No of cats : {obj.nCats} and No of dogs : {obj.nDogs} \")","metadata":{"execution":{"iopub.status.busy":"2024-04-17T08:42:32.374916Z","iopub.execute_input":"2024-04-17T08:42:32.375462Z","iopub.status.idle":"2024-04-17T08:42:32.382351Z","shell.execute_reply.started":"2024-04-17T08:42:32.375422Z","shell.execute_reply":"2024-04-17T08:42:32.381311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data = np.load(\"train_data.npy\", allow_pickle = True)\ntraining_data.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-17T08:42:34.206588Z","iopub.execute_input":"2024-04-17T08:42:34.207450Z","iopub.status.idle":"2024-04-17T08:42:34.560552Z","shell.execute_reply.started":"2024-04-17T08:42:34.207404Z","shell.execute_reply":"2024-04-17T08:42:34.558877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(training_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-17T08:52:29.292381Z","iopub.execute_input":"2024-04-17T08:52:29.293013Z","iopub.status.idle":"2024-04-17T08:52:29.301383Z","shell.execute_reply.started":"2024-04-17T08:52:29.292977Z","shell.execute_reply":"2024-04-17T08:52:29.299493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data[0][0]","metadata":{"execution":{"iopub.status.busy":"2024-04-17T08:53:10.399772Z","iopub.execute_input":"2024-04-17T08:53:10.400145Z","iopub.status.idle":"2024-04-17T08:53:10.409316Z","shell.execute_reply.started":"2024-04-17T08:53:10.400116Z","shell.execute_reply":"2024-04-17T08:53:10.407774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(training_data[0][0])","metadata":{"execution":{"iopub.status.busy":"2024-04-17T08:53:44.613794Z","iopub.execute_input":"2024-04-17T08:53:44.614183Z","iopub.status.idle":"2024-04-17T08:53:44.833831Z","shell.execute_reply.started":"2024-04-17T08:53:44.614154Z","shell.execute_reply":"2024-04-17T08:53:44.832306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch-vision","metadata":{"execution":{"iopub.status.busy":"2024-04-17T08:56:54.745542Z","iopub.execute_input":"2024-04-17T08:56:54.746019Z","iopub.status.idle":"2024-04-17T08:57:12.967033Z","shell.execute_reply.started":"2024-04-17T08:56:54.745978Z","shell.execute_reply":"2024-04-17T08:57:12.965395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(2)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T09:41:47.731084Z","iopub.execute_input":"2024-04-17T09:41:47.731557Z","iopub.status.idle":"2024-04-17T09:41:47.737985Z","shell.execute_reply.started":"2024-04-17T09:41:47.731515Z","shell.execute_reply":"2024-04-17T09:41:47.736888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Torch libraries\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-04-17T09:01:26.291984Z","iopub.execute_input":"2024-04-17T09:01:26.292528Z","iopub.status.idle":"2024-04-17T09:01:26.299648Z","shell.execute_reply.started":"2024-04-17T09:01:26.292488Z","shell.execute_reply":"2024-04-17T09:01:26.298057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels = 1, out_channels =32, kernel_size =5)\n        # if input image is grayscale/BW then in_channels =1, \n        # else if RBG then in_channels = 3\n        # The out_channels is not single filter, In a single layer it is the no of filters \n        # applied on same image, again and again, in all window positions.\n        \n        # At each position of the input image, a single filter (or kernel) slides over the receptive field (a window of the input) \n        # and performs a convolution operation.\n        # The result of this convution operation at each position produces a single value in the output feature map.\n        # This process is repeated for each filter in the layer.\n        \n        # Multiple Filters:\n        # Each convolutional layer consists of multiple filters, where the number of filters is specified by \n        # the out_channels parameter.\n        # Each filter slides over the entire input image independently.\n        # Each filter learns to detect different features or patterns from the input image, such as edges, textures, or shapes.\n        # The output of each filter applied to the input image produces a separate feature map.\n        \n        # kernel_size is the windows size of the kernel e.g. (5x5)\n        # Note that stride is also a parameter, indicating by how many pixels should window be shifted to next position \n        # after convultion at each position\n        \n        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5)\n        \n        # Note that in the 2nd layer, the in_channels is actually (no of filters * no of channels) used in previous layer.\n        # Since no of channels in prev layer is 1, hence in_channels in conv2 is simply = no filters in prev layer.\n        # Now we can again define, the number out_channels , no of filters in 2nd layer;\n        \n        # But first, to get the output of 1st layer, we need to define maxpool and activation fn, whose output is the 2nd layer input tensor\n        # We can apply below, or use these functions from F in the fly.\n        # self.relu = nn.ReLu()\n        # self.maxpool = F.max_pool2d()\n        # Note that Applying activation function to the output of a max pooling operation does not change the spatial dimensions of the feature maps.\n        \n        # Next we need to define the Fully Connected layers, since we want a vector or single value for classification, \n        \n        self.fc1 = nn.Linear( 12*12 * 64, 256)\n        \n        # Note that to define fc after Conv, we need to know the shape of output of final max_pool. \n        # In keras we simply use flatten() func, but in Pytorch manual calculation must be done\n        \n        # For our problem, image h,w = 50,50 and kernel of conv = 5x5.\n        # Assume maxpool we define as max_pool2d(input, kernel_size = 2)\n        # Typically we define kernel for maxpool as 2x2 or 3x3. Assume we define 2x2\n        # But another parameter of maxpool is stride,if you don't specify the stride parameter when calling, it will default to the kernel_size. \n        # Hence, maxpool stride = 2x2. \n        \n        #So output of 1st maxpool is 25x25. \n        \n        # similarly when we apply 2nd conv which again gives 25x25 output which will be passed to 2nd maxpool,\n        # The 2nd maxpool gives 12x12 (it ignores last column of 25 columns).\n        \n        # Hence the 12*12 * 64 is the size of total output of 2nd maxpool considering all filter channels also.\n        # Now simply define other FC layers.\n         \n        self.fc2 = nn.Linear(256, 2) #since 2 output class\n        \n    def forward(self, x):\n        x = F.relu(conv1(x))\n        x = F.max_pool2d(x, kernel_size = 2)\n        x = F.relu(conv2(x))\n        x = F.max_pool2d(x, kernel_size = 2)\n        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-17T11:18:08.254273Z","iopub.execute_input":"2024-04-17T11:18:08.255732Z","iopub.status.idle":"2024-04-17T11:18:08.270269Z","shell.execute_reply.started":"2024-04-17T11:18:08.255669Z","shell.execute_reply":"2024-04-17T11:18:08.268834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MyCNN()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T11:18:33.247310Z","iopub.execute_input":"2024-04-17T11:18:33.247755Z","iopub.status.idle":"2024-04-17T11:18:33.322595Z","shell.execute_reply.started":"2024-04-17T11:18:33.247723Z","shell.execute_reply":"2024-04-17T11:18:33.321172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can find the shape of input to be passed to FC layers by passing random input values to initial layers\n# and getting the shape of their output, which is desired shape\n\n# This process is commonly referred to as \"shape inference\" or \"forward pass inference\"\n\n#SO lets make a TestCNN, for this \n\n# FORWARD PASS INFERENCE\n\nclass TestCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 5)\n        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5)\n    \n    def forward(self, x):\n        x = F.relu( self.conv1(x))\n        x = F.max_pool2d(x, kernel_size = 2)\n        x = F.relu( self.conv2(x))\n        x = F.max_pool2d(x, kernel_size = 2)\n        return x\ntestM = TestCNN()\nprint(testM)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T12:11:56.370439Z","iopub.execute_input":"2024-04-17T12:11:56.370881Z","iopub.status.idle":"2024-04-17T12:11:56.385293Z","shell.execute_reply.started":"2024-04-17T12:11:56.370849Z","shell.execute_reply":"2024-04-17T12:11:56.383379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now Lets make a dummy input x, Remember input to conv2d is of form\n# (Batchsize ie no images in a batch, no of channels, height of image, width of image)\n# Hence lets define single image batch with only 1 image\n\nx = torch.randn(1,1,50,50)\n\n# Note that same can be achieved with higher batch size, especially during actuall implementation\n# using view(-1,other dimesions) command\n \n# x = torch.randn(50,50).view(-1,1,50,50) \n#In this case batch size is infered from the shape of input sent and other dimensions\n\noutput = testM(x)\n\nprint(output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T12:12:02.679237Z","iopub.execute_input":"2024-04-17T12:12:02.679640Z","iopub.status.idle":"2024-04-17T12:12:02.690894Z","shell.execute_reply.started":"2024-04-17T12:12:02.679611Z","shell.execute_reply":"2024-04-17T12:12:02.689235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So our earlier Analysis of the architeture is wrong. ","metadata":{"execution":{"iopub.status.busy":"2024-04-17T12:12:09.829629Z","iopub.execute_input":"2024-04-17T12:12:09.830130Z","iopub.status.idle":"2024-04-17T12:12:09.836537Z","shell.execute_reply.started":"2024-04-17T12:12:09.830094Z","shell.execute_reply":"2024-04-17T12:12:09.834732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The architecture of the CNN is as follows:\n\nThe input size is 50×50 pixels.\nThe first convolutional layer (conv1) has 32 output channels and a kernel size of \n5×5.\n\nThe second convolutional layer (conv2) has 64 output channels and a kernel size of \n5×5.\n\nBetween each convolutional layer, there is a max pooling operation with a kernel size of \n2×2.\n\nIMP : Note that Output of conv2d layer is not same size as size of input img. \n\n Formula with zero padding  : Output size = ⌊(Input size−Kernel size) / Stride⌋ +1\n                                            ⌊( 50 − 5) / 1⌋ +1 = 46\n\nThis can also be logically understood since, as we slider the kernel, \nfor the last 4 columns, the window doesnt move. Hence output of conv1 is 46x46\n ","metadata":{}},{"cell_type":"code","source":"# Next few layers analysis : ","metadata":{"execution":{"iopub.status.busy":"2024-04-17T12:12:19.676281Z","iopub.execute_input":"2024-04-17T12:12:19.676721Z","iopub.status.idle":"2024-04-17T12:12:19.682787Z","shell.execute_reply.started":"2024-04-17T12:12:19.676689Z","shell.execute_reply":"2024-04-17T12:12:19.681279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the first max pooling operation:\n\nOutput size: 23×23 (since the default stride is equal to the kernel size)\n\nAfter the second convolutional layer (conv2):\n\nOutput size: 19×19 (since (23-5)/1 + 1 = 19, or since (19,20,21,22,23) is the last window. We cant shift further)\n\nAfter the second max pooling operation:\n\nOutput size: 9×9 (since we ignore last single col)","metadata":{}},{"cell_type":"code","source":"# Anyway Lets Move on. and Define correct CNN architecure.\nclass MyCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels = 1, out_channels =32, kernel_size =5)\n        \n        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5)\n        \n        self.fc1 = nn.Linear( 9 * 9 * 64, 256)\n        \n        self.fc2 = nn.Linear(256, 2) #since 2 output class\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size = 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size = 2)\n        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.softmax(self.fc2(x), dim = 1) # Last layer. So act = softmax, since binary classify dim =1\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:03:25.620968Z","iopub.execute_input":"2024-04-17T14:03:25.621481Z","iopub.status.idle":"2024-04-17T14:03:25.633578Z","shell.execute_reply.started":"2024-04-17T14:03:25.621446Z","shell.execute_reply":"2024-04-17T14:03:25.631974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MyCNN()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:03:26.707114Z","iopub.execute_input":"2024-04-17T14:03:26.708133Z","iopub.status.idle":"2024-04-17T14:03:26.731087Z","shell.execute_reply.started":"2024-04-17T14:03:26.708093Z","shell.execute_reply":"2024-04-17T14:03:26.729425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get Input featureset X and target y\nX = []\ny = []\nfor i in training_data:\n    X.append(i[0])\n    y.append(i[1])\n#X = np.array(X)\n#y = np.array(y)\nX = torch.tensor(np.array(X))\ny = torch.tensor(np.array(y))\nX = X / 255.0","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:03:28.518138Z","iopub.execute_input":"2024-04-17T14:03:28.518618Z","iopub.status.idle":"2024-04-17T14:03:29.049712Z","shell.execute_reply.started":"2024-04-17T14:03:28.518584Z","shell.execute_reply":"2024-04-17T14:03:29.047988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.dtype","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:03:30.347673Z","iopub.execute_input":"2024-04-17T14:03:30.348115Z","iopub.status.idle":"2024-04-17T14:03:30.356608Z","shell.execute_reply.started":"2024-04-17T14:03:30.348083Z","shell.execute_reply":"2024-04-17T14:03:30.354890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:03:31.154780Z","iopub.execute_input":"2024-04-17T14:03:31.155186Z","iopub.status.idle":"2024-04-17T14:03:31.164833Z","shell.execute_reply.started":"2024-04-17T14:03:31.155155Z","shell.execute_reply":"2024-04-17T14:03:31.163143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:03:31.846971Z","iopub.execute_input":"2024-04-17T14:03:31.847476Z","iopub.status.idle":"2024-04-17T14:03:31.859744Z","shell.execute_reply.started":"2024-04-17T14:03:31.847441Z","shell.execute_reply":"2024-04-17T14:03:31.858247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[0].view(1,50,50)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:18:54.680513Z","iopub.execute_input":"2024-04-17T14:18:54.680953Z","iopub.status.idle":"2024-04-17T14:18:54.692629Z","shell.execute_reply.started":"2024-04-17T14:18:54.680921Z","shell.execute_reply":"2024-04-17T14:18:54.691216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:03:32.676873Z","iopub.execute_input":"2024-04-17T14:03:32.677425Z","iopub.status.idle":"2024-04-17T14:03:32.686941Z","shell.execute_reply.started":"2024-04-17T14:03:32.677386Z","shell.execute_reply":"2024-04-17T14:03:32.685353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Test Split\n\ntestSplit = 0.1\ntestSize = int(len(X)*0.1)\ntrainSize = len(X) - testSize\n\nXtrain = X[:trainSize]\nXtest = X[trainSize:]\n\nytrain = y[:trainSize]\nytest = y[trainSize:]","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:03:33.021444Z","iopub.execute_input":"2024-04-17T14:03:33.022922Z","iopub.status.idle":"2024-04-17T14:03:33.031009Z","shell.execute_reply.started":"2024-04-17T14:03:33.022876Z","shell.execute_reply":"2024-04-17T14:03:33.029232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtrain[0:32].view(-1,1,50,50).shape # take watver is the batch_size but make sure channel is kept as 1, rest H&W","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:03:33.371745Z","iopub.execute_input":"2024-04-17T14:03:33.372214Z","iopub.status.idle":"2024-04-17T14:03:33.382167Z","shell.execute_reply.started":"2024-04-17T14:03:33.372164Z","shell.execute_reply":"2024-04-17T14:03:33.380978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the sample output of model\n\nmodel(X[0].view(1,1,50,50))","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:19:14.625337Z","iopub.execute_input":"2024-04-17T14:19:14.625884Z","iopub.status.idle":"2024-04-17T14:19:14.640058Z","shell.execute_reply.started":"2024-04-17T14:19:14.625850Z","shell.execute_reply":"2024-04-17T14:19:14.638568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:21:04.894863Z","iopub.execute_input":"2024-04-17T14:21:04.895379Z","iopub.status.idle":"2024-04-17T14:21:04.906826Z","shell.execute_reply.started":"2024-04-17T14:21:04.895346Z","shell.execute_reply":"2024-04-17T14:21:04.905030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"code","source":"epochs = 20 \nlr = 0.01\n\nbatch_size = 32\n\nopt = torch.optim.Adam( model.parameters(), lr = 0.00)\nmse = nn.CrossEntropyLoss()\n\nval_loss_min = np.inf\nearly_stop_cnt = 0\n\nfor ep in range(epochs):\n    for i in tqdm(range(0,len(Xtrain),batch_size)): # Go from 0 to entire length leaving gap of batch_size\n        Xtrain_batched = Xtrain[i:i+batch_size].view(-1,1,50,50)\n        ytrain_batched = ytrain[i:i+batch_size]\n        #print(Xtrain_batched.shape)\n        \n        # Xtrain_batched = Xtrain_batched.to(torch.float32)\n        \n        opt.zero_grad()\n        \n        model.train()\n        \n        ypred = model(Xtrain_batched)\n        \n        loss = mse(ypred, ytrain_batched)\n        \n        loss.backward()\n        \n        opt.step()\n        \n    with torch.no_grad():\n        model.eval()\n        \n        Xtest = Xtest.view(-1,1,50,50)\n        \n        output_val = model(Xtest)\n        \n        val_loss = mse(output_val, ytest)\n        \n        print(f\" epoch : {ep}, training_loss : {train_loss}\")\n        \n        if val_loss < val_loss_min:\n            print(f\"val Loss decreased. {val_loss_min} -> {val_loss} \")\n            val_loss_min = val_loss\n            early_stop_cnt = 0\n        else:\n            print('val loss same')\n            early_stop_cnt +=1\n        print(f\" Best val loss : {val_loss_min}\")\n        \n        if early_stop_cnt == 10:\n            print(\" Early Stopping ); \")\n            break\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:42:00.830024Z","iopub.execute_input":"2024-04-17T14:42:00.831249Z","iopub.status.idle":"2024-04-17T14:53:53.487973Z","shell.execute_reply.started":"2024-04-17T14:42:00.831174Z","shell.execute_reply":"2024-04-17T14:53:53.486593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_index = 100\n\nplt.imshow(X[test_index])","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:41:42.190558Z","iopub.execute_input":"2024-04-17T14:41:42.191622Z","iopub.status.idle":"2024-04-17T14:41:42.466522Z","shell.execute_reply.started":"2024-04-17T14:41:42.191584Z","shell.execute_reply":"2024-04-17T14:41:42.465517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = model(X[test_index].view(1,1,50,50))\nprint(y[test_index])\nprint((out))","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:41:43.976067Z","iopub.execute_input":"2024-04-17T14:41:43.977673Z","iopub.status.idle":"2024-04-17T14:41:43.990671Z","shell.execute_reply.started":"2024-04-17T14:41:43.977633Z","shell.execute_reply":"2024-04-17T14:41:43.988466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}